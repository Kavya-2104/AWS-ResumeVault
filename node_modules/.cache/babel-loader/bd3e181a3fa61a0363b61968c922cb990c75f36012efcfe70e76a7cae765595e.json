{"ast":null,"code":"import '@smithy/md5-js';\nimport '@aws-amplify/core/internals/aws-client-utils';\nimport '../../utils/client/runtime/s3TransferHandler/fetch.mjs';\nimport 'fast-xml-parser';\nimport '../../utils/client/runtime/s3TransferHandler/xhr.mjs';\nimport 'buffer';\nimport '@aws-amplify/core/internals/utils';\nimport { assertValidationError } from '../../../../errors/utils/assertValidationError.mjs';\nimport { StorageValidationErrorCode } from '../../../../errors/types/validation.mjs';\nimport { MAX_OBJECT_SIZE, DEFAULT_PART_SIZE } from '../../utils/constants.mjs';\nimport { createUploadTask } from '../../utils/transferTask.mjs';\nimport { byteLength } from './byteLength.mjs';\nimport { putObjectJob } from './putObjectJob.mjs';\nimport { getMultipartUploadHandlers } from './multipart/uploadHandlers.mjs';\n\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Upload data to specified S3 object. By default, it uses single PUT operation to upload if the data is less than 5MB.\n * Otherwise, it uses multipart upload to upload the data. If the data length is unknown, it uses multipart upload.\n *\n * Limitations:\n * * Maximum object size is 5TB.\n * * Maximum object size if the size cannot be determined before upload is 50GB.\n *\n * @param input - The UploadDataInput object.\n * @returns A cancelable and resumable task exposing result promise from `result`\n * \tproperty.\n * @throws service: {@link S3Exception} - thrown when checking for existence of the object\n * @throws validation: {@link StorageValidationErrorCode } - Validation errors.\n *\n * @example\n * ```ts\n * // Upload a file to s3 bucket\n * await uploadData({ key, data: file, options: {\n *   onProgress, // Optional progress callback.\n * } }).result;\n * ```\n * @example\n * ```ts\n * // Cancel a task\n * const uploadTask = uploadData({ key, data: file });\n * //...\n * uploadTask.cancel();\n * try {\n *   await uploadTask.result;\n * } catch (error) {\n *   if(isCancelError(error)) {\n *     // Handle error thrown by task cancelation.\n *   }\n * }\n *```\n *\n * @example\n * ```ts\n * // Pause and resume a task\n * const uploadTask = uploadData({ key, data: file });\n * //...\n * uploadTask.pause();\n * //...\n * uploadTask.resume();\n * //...\n * await uploadTask.result;\n * ```\n */\nconst uploadData = input => {\n  const {\n    data\n  } = input;\n  const dataByteLength = byteLength(data);\n  assertValidationError(dataByteLength === undefined || dataByteLength <= MAX_OBJECT_SIZE, StorageValidationErrorCode.ObjectIsTooLarge);\n  if (dataByteLength && dataByteLength <= DEFAULT_PART_SIZE) {\n    const abortController = new AbortController();\n    return createUploadTask({\n      isMultipartUpload: false,\n      job: putObjectJob(input, abortController.signal, dataByteLength),\n      onCancel: message => {\n        abortController.abort(message);\n      }\n    });\n  } else {\n    const {\n      multipartUploadJob,\n      onPause,\n      onResume,\n      onCancel\n    } = getMultipartUploadHandlers(input, dataByteLength);\n    return createUploadTask({\n      isMultipartUpload: true,\n      job: multipartUploadJob,\n      onCancel: message => {\n        onCancel(message);\n      },\n      onPause,\n      onResume\n    });\n  }\n};\nexport { uploadData };","map":{"version":3,"names":["uploadData","input","data","dataByteLength","byteLength","assertValidationError","undefined","MAX_OBJECT_SIZE","StorageValidationErrorCode","ObjectIsTooLarge","DEFAULT_PART_SIZE","abortController","AbortController","createUploadTask","isMultipartUpload","job","putObjectJob","signal","onCancel","message","abort","multipartUploadJob","onPause","onResume","getMultipartUploadHandlers"],"sources":["E:\\CSC-PROJECT\\amplify-react-app\\node_modules\\@aws-amplify\\storage\\src\\providers\\s3\\apis\\uploadData\\index.ts"],"sourcesContent":["// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { createUploadTask } from '../../utils';\nimport { assertValidationError } from '../../../../errors/utils/assertValidationError';\nimport { StorageValidationErrorCode } from '../../../../errors/types/validation';\nimport { DEFAULT_PART_SIZE, MAX_OBJECT_SIZE } from '../../utils/constants';\nimport { byteLength } from './byteLength';\nimport { putObjectJob } from './putObjectJob';\nimport { getMultipartUploadHandlers } from './multipart';\n/**\n * Upload data to specified S3 object. By default, it uses single PUT operation to upload if the data is less than 5MB.\n * Otherwise, it uses multipart upload to upload the data. If the data length is unknown, it uses multipart upload.\n *\n * Limitations:\n * * Maximum object size is 5TB.\n * * Maximum object size if the size cannot be determined before upload is 50GB.\n *\n * @param input - The UploadDataInput object.\n * @returns A cancelable and resumable task exposing result promise from `result`\n * \tproperty.\n * @throws service: {@link S3Exception} - thrown when checking for existence of the object\n * @throws validation: {@link StorageValidationErrorCode } - Validation errors.\n *\n * @example\n * ```ts\n * // Upload a file to s3 bucket\n * await uploadData({ key, data: file, options: {\n *   onProgress, // Optional progress callback.\n * } }).result;\n * ```\n * @example\n * ```ts\n * // Cancel a task\n * const uploadTask = uploadData({ key, data: file });\n * //...\n * uploadTask.cancel();\n * try {\n *   await uploadTask.result;\n * } catch (error) {\n *   if(isCancelError(error)) {\n *     // Handle error thrown by task cancelation.\n *   }\n * }\n *```\n *\n * @example\n * ```ts\n * // Pause and resume a task\n * const uploadTask = uploadData({ key, data: file });\n * //...\n * uploadTask.pause();\n * //...\n * uploadTask.resume();\n * //...\n * await uploadTask.result;\n * ```\n */\nexport const uploadData = (input) => {\n    const { data } = input;\n    const dataByteLength = byteLength(data);\n    assertValidationError(dataByteLength === undefined || dataByteLength <= MAX_OBJECT_SIZE, StorageValidationErrorCode.ObjectIsTooLarge);\n    if (dataByteLength && dataByteLength <= DEFAULT_PART_SIZE) {\n        const abortController = new AbortController();\n        return createUploadTask({\n            isMultipartUpload: false,\n            job: putObjectJob(input, abortController.signal, dataByteLength),\n            onCancel: (message) => {\n                abortController.abort(message);\n            },\n        });\n    }\n    else {\n        const { multipartUploadJob, onPause, onResume, onCancel } = getMultipartUploadHandlers(input, dataByteLength);\n        return createUploadTask({\n            isMultipartUpload: true,\n            job: multipartUploadJob,\n            onCancel: (message) => {\n                onCancel(message);\n            },\n            onPause,\n            onResume,\n        });\n    }\n};\n"],"mappings":";;;;;;;;;;;;;;;AAAA;AACA;AAQA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACY,MAACA,UAAU,GAAIC,KAAK,IAAK;EACjC,MAAM;IAAEC;EAAI,CAAE,GAAGD,KAAK;EACtB,MAAME,cAAc,GAAGC,UAAU,CAACF,IAAI,CAAC;EACvCG,qBAAqB,CAACF,cAAc,KAAKG,SAAS,IAAIH,cAAc,IAAII,eAAe,EAAEC,0BAA0B,CAACC,gBAAgB,CAAC;EACrI,IAAIN,cAAc,IAAIA,cAAc,IAAIO,iBAAiB,EAAE;IACvD,MAAMC,eAAe,GAAG,IAAIC,eAAe,EAAE;IAC7C,OAAOC,gBAAgB,CAAC;MACpBC,iBAAiB,EAAE,KAAK;MACxBC,GAAG,EAAEC,YAAY,CAACf,KAAK,EAAEU,eAAe,CAACM,MAAM,EAAEd,cAAc,CAAC;MAChEe,QAAQ,EAAGC,OAAO,IAAK;QACnBR,eAAe,CAACS,KAAK,CAACD,OAAO,CAAC;MAC9C;IACA,CAAS,CAAC;EACV,CAAK,MACI;IACD,MAAM;MAAEE,kBAAkB;MAAEC,OAAO;MAAEC,QAAQ;MAAEL;IAAQ,CAAE,GAAGM,0BAA0B,CAACvB,KAAK,EAAEE,cAAc,CAAC;IAC7G,OAAOU,gBAAgB,CAAC;MACpBC,iBAAiB,EAAE,IAAI;MACvBC,GAAG,EAAEM,kBAAkB;MACvBH,QAAQ,EAAGC,OAAO,IAAK;QACnBD,QAAQ,CAACC,OAAO,CAAC;MACjC,CAAa;MACDG,OAAO;MACPC;IACZ,CAAS,CAAC;EACV;AACA"},"metadata":{},"sourceType":"module","externalDependencies":[]}